Hi, everyone. Good day. We are here to present our POC PRO V for the Aws January T 2024. Let me hand it over to my team member Vishal to explain the architecture diagram of the same. Thank you. Very pro is a end to end RG product where the user can retrieve the project information with just the natural language chatting with the chatbot. Now we will go to the architecture in architecture. We follow two phase architecture. Phase one is knowledge based creation and phase two is chat pipeline in phase one, we take user input and convert that into a knowledge base user input. Here we speak. This video recording, video recording uh will consist of meeting calls, client calls or internal uh employee uh conversations when problem tickets and incident tickets and design documents. Design documents will be in the format of document PDF or Excel. It can be anything a 2nd 2nd module is the data injection for video recording data ingestion will be like we inject the MP four format or MP3 audio format and we will pass that into Amazon transcribe solution which will provide us the transcript of the audio or the video. And for problem ticket and design document, we just ingest the doc format or P format or Excel format. After that, we will go to the data preprocessing layer, data prep crossing layer, we will chunk the data which is the text data into into a various chunk. We can define the chunk size like 5 to 1000 24. It's our option. After that, we will pass that chunk into the Amazon Titan Embody which will embed the text chunks into a vector which consist of Numericals. After converting that into vector, we will index the vectors into a app uh W TB. After indexing, we create a vector store out of it which will consist of all the vector in into a 3d 3d space. This is the first phase of the pipeline. The second phase of the pipeline is chart pipeline. Here we get the user query. Uh We use a query from the front end and we will do a similarity search with the vector store which is stored in the first face of the architecture which will the similarity search will retrieve top five similar information which is inside the vector store. And next, we will pass that knowledge chunks query to the Amazon Titan Express. We will set up a system prompt like uh this is uh the user query is so and so, and we have the information related to the query and the Amazon Titan express will figure out the natural language response out of it. Now, I will hand out now I will hand out to Pushkar. Yeah, thank you, Visa. So let's go with the uh aws service used. Uh We use four aws servers. Uh The first one is easy two, the elastic computed compute cloud is a flexible cloud computing capacity where we deploy the application. The second one is three, it is a simple storage service which is scalable and reliable. It is used to store the input files and V DB details. The next one we use is Amazon transcribe, which is used for transcript, the meeting recordings or call recordings to text format and the food service we use is Amazon Bedrock Titan. It is a comprehensive suite of foundation models and tools with text inverting and text express. So let's go the cost overview over to us. Thank you for waiting. Here. We have four Amazon services. First one is Amazon transcribe. We use around $6. This is very cost efficient solution to transcript and audio or video. We we use around $6 and next one is Amazon Petro. Here we use Amazon Titan Foundation model so called on 17 university. And then we use easy two instance, easy two instance, it is around $30. But this is optional because we are planning to use a streamlet, which is a Python library which you can use to develop front end and back end of the chat application they provide a free instance just called the streamlet cloud where we can deploy our streamlet application for free. If you opt for that, then we don't need this 30 inch or also you need this. And next one is the S3 here. We will store it, we store the user uh information, which is that the documents or video meeting recordings about. And we will restore the chat history and we stored the vector DB here which will cost around a dollar because uh uh for $25 we use $25 but it may vary in future. I will, I will learn to GT. Thank you, Michelle. Let me talk about the business impact of this model. So, retrieval procedures that are incorporated in this model helps an 85% faster retrieval of data which increases the productivity and enables the rapid decision meeting. The automation process infused that trims the repetitive process which reduces the 25% of the resource cost. The insights to make decisions which are comprehensive, improves the interaction and provides 100% security. As this is a centralized knowledge repository, it provides 30% collaboration which in turn enhances training efficacy of new employees. Hand it over to guide through for the ability and customer benefit. Thank you JT. Let me explain about the reusability and customer benefit. So the first one is development time. There will be a decrease in the development time because it will be a plug and way model where we will be obviously having the framework of the project information and thereby it increases the customization. So coming to the business transformation, we will be definitely having a 50% decrease in the business transition time because obviously we will be having all the information handy. So when we transition that to a different project or a team, there will there will be a very less transition time because of the same coming to the customer benefit. So the first one is project delivery, there will be an increase in the project delivery around 30% because since it is a live information hub where we used to provide this centralized information and we will be having always the confident responses from our intelligent project assistant coming to the accuracy. So which with all these features, we will be definitely having a very high accuracy which thereby increases the information reliability also and security as well. So let me hand it over to Vishal for a uh providing the future improvements of this POC. Yeah, thank you. I would like to add a few points on future improvements which we can develop on next ration. First feature is access management, we can restrict particular users from accessing specific information. This can be achieved by providing dedicated knowledge base to each user even though it will cost a bit in storage side. But the data integrity will be maintained. The second feature is chat retention. We can add the ability to store the chat history to access the chat context for later use, which will save time to set system prompt as we do in Chad, GPT and Germany. And the third feature I would like to propose is the recommendations. It is adding the ability to make recommendation which will enhance decision making and efficiency. For example, we can get the information about a problem ticket and the recommendation to solve the problem. And the fourth and final feature is logging system, we can log the interaction between chatbot and the user. This feature will enhance accountability and transparency which is crucial for regulatory compliances and internal governance. And there are, there are there are endless features that can be added to this chatbot to improve the efficiency. Thank you.